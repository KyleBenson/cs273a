%%
%% Class homework & solution template for latex
%% Alex Ihler
%%
\documentclass[twoside,11pt]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{graphicx,color}
\usepackage{verbatim,url}
\usepackage{listings}
\usepackage{upquote}
\usepackage[T1]{fontenc}
%\usepackage{lmodern}
\usepackage[scaled]{beramono}
\usepackage{enumerate}
\usepackage{float}
%\usepackage{textcomp}

% Directories for other source files and images
\newcommand{\bibtexdir}{../bib}
\newcommand{\figdir}{figs}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\matlab}{{\sc Matlab}\ }

\setlength{\textheight}{9in} \setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{-.25in}  % Centers text.
\setlength{\evensidemargin}{-.25in} %
\setlength{\topmargin}{0in} %
\setlength{\headheight}{0in} %
\setlength{\headsep}{0in} %

\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\arabic{enumii})}

\theoremstyle{definition}
\newtheorem{MatEx}{M{\scriptsize{ATLAB}} Usage Example}

\definecolor{comments}{rgb}{0,.5,0}
\definecolor{backgnd}{rgb}{.95,.95,.95}
\definecolor{string}{rgb}{.2,.2,.2}
\lstset{language=Matlab}
\lstset{basicstyle=\small\ttfamily,
        mathescape=true,
        emptylines=1, showlines=true,
        backgroundcolor=\color{backgnd},
        commentstyle=\color{comments}\ttfamily, %\rmfamily,
        stringstyle=\color{string}\ttfamily,
        keywordstyle=\ttfamily, %\normalfont,
        showstringspaces=false}
\newcommand{\matp}{\mathbf{\gg}}




\begin{document}

\centerline{\Large Kyle Benson}
\centerline{CS 273A - Machine Learning: Fall 2013}
\centerline{Homework 2}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\subsection*{Problem 1: Linear Regression}

\begin{enumerate}[(a)]
\item Done
\item \vspace{-1in}
\includegraphics[width=0.5\textwidth]{\figdir/prob1b.pdf} \\
Training MSE: 2235.8 \\
Test MSE: 2414.7

\item 

\end{enumerate}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\subsection*{Problem 2: Perceptron Classifiers}

\begin{enumerate}[(a)]
\item classes 0 vs. 1 \\
\includegraphics{\figdir/prob2a_0v1} \\
classes 1 vs. 2 \\
\includegraphics{\figdir/prob2a_1v2} \\

\item \lstinputlisting{src/prob2.m}

\item TODO
\includegraphics{\figdir/prob2c_0v1.pdf} \\
\includegraphics{\figdir/prob2c_1v2.pdf}

\item TODO
\includegraphics{\figdir/prob2d_0v1.pdf} \\
\includegraphics{\figdir/prob2d_1v2.pdf}

\item TODO
\includegraphics{\figdir/prob2e_linear.pdf} \\
%\includegraphics{\figdir/prob2e_perceptron.pdf}

\end{enumerate}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\subsection*{Problem 3: Logistic Regresion}

\begin{enumerate}[(a)]
\item Done
\item See (c)


\item $\frac{dJ(\theta)}{d\theta} = \frac{d}{d\theta} (\frac{1}{m} \sum_{j} -y^{(j)} \log\sigma(\theta x^{(j)T}) - (1-y^{(j)}) \log(1 - \sigma(\theta x^{(j)T})))$ \\

$= \frac{1}{m} \sum_{j} -\frac{d}{d\theta}y^{(j)} \log\sigma(\theta x^{(j)T}) - \frac{d}{d\theta}(1-y^{(j)}) \log(1 - \sigma(\theta x^{(j)T})))$ \\
$= \frac{1}{m} \sum_{j} -\frac{\frac{d}{d\theta} \sigma(\theta x^{(j)T}) y^{(j)}}{\sigma(\theta x^{(j)T})} - y^{(j)} \frac{-\frac{d}{d\theta} \sigma(\theta x^{(j)T}) y^{(j)}}{1 - \sigma(\theta x^{(j)T})}$ \\

\end{enumerate}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

%
%\subsection*{Problem 2: Decision Trees}
%
%\begin{enumerate}[(a)]
%
%\item $H(y) = - 0.4 * log(0.4) - 0.6*log(0.6) = 0.971$
%
%\item Using the formula $H(y|x_i = 0) = p(y=1 | x_i = 0) \log p(y=1 | x_i = 0) + p(y=0 | x_i = 0) \log p(y=0 | x_i = 0)$
%\begin{tabular}{|c|c|c|c|c|c|}\hline
%$Variable$ & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ \\
%\hline
%Entropy $H(y|x_i = 0)$ & 1 & 0.4312 & 1.03 & 0.931 & 0.887 \\
%\hline
%Entropy $H(y|x_i = 1)$ & 0.811 & 0.22 & 0.701 & 0.72 & 1.028 \\
%\hline
%Info Gain &  \\
%\hline
%\end{tabular}
%\\ Therefore, we should split on feature $x_?$ first.
%
%%\item \includegraphics{problem2_tree}
%\end{enumerate}
%
%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%
%\subsection*{Problem 2: K-Nearest Neighbors and Validation}
%
%\begin{enumerate}[(a)]
%\item 
%\begin{figure}[H] \centering
%\begin{tabular}{cccc}
%\includegraphics[width=.22\textwidth]{\figdir/problem2_K1} &
%\includegraphics[width=.22\textwidth]{\figdir/problem2_K5} &
%\includegraphics[width=.22\textwidth]{\figdir/problem2_K10} &
%\includegraphics[width=.22\textwidth]{\figdir/problem2_K50} \\
%$K=1$ & $K=5$ & $K=10$ & $K=50$ \\
%\end{tabular}
%\end{figure}
%
%\item 
%\begin{figure}[H] \centering
%Given the following MSEs: \\
%\includegraphics[width=6.5in]{\figdir/prob2b} \\
%I would recommend choosing $K=16$ since the MSE appears to increase for the test set after this value.
%\end{figure}
%
%\item 
%\begin{figure}[H] \centering
%Given the following MSEs: \\
%\includegraphics[width=6.5in]{\figdir/prob2c} \\
%I would recommend choosing $K=32$ since the MSE appears to increase for the test set after this value.
%\end{figure}
%
%\end{enumerate}
%
%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%
%\subsection*{Problem 3: Bayes Classifiers}
%
%\begin{enumerate}[(a)]
%	\item See plot in part c
%	\item See plot in part c
%	
%	\item
%		\begin{figure}[H] \centering
%		\includegraphics[width=6.5in]{\figdir/prob3a} \\
%		\end{figure}
%		
%	\item
%		\begin{figure}[H] \centering
%		\includegraphics[width=6.5in]{\figdir/prob3b} \\
%		\end{figure}
%
%	\item training error rate $=$ 0.2252 \\
%		test error rate $=$ 0.5676
%	
%	\item training error rate $=$ 0.0360 \\
%		test error rate $=$ 0.0541
%	
%\end{enumerate}
%
%
%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%
%\subsection*{Problem 4: Decision Trees}
%
%\begin{enumerate}[(a)]
%	\item Done
%	
%	\item
%		\begin{figure}[H] \centering
%		\includegraphics[width=6.5in]{\figdir/prob4} \\
%		\end{figure}
%		
%	\item For 2 features, I would choose to limit the depth to 3.  After several runs, very few seem to improve the misclassification rate beyond a depth of 3 and many runs seem to actually worsen the misclassification rate beyond a depth of 3. Here, the test error comes to around 0.27.
%
%	\item For 4 features, I would choose to limit the depth to 3.  Similarly to the 2 feature case, few runs seem to improve beyond this depth.
%Although very few runs actually get worse beyond this depth, many of them show an improvement between depth 2 and 3, so the extra complexity seems to usually be worth it.  Here, the test error comes to between 0 and 0.1.
%
%\end{enumerate}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\end{document}
